# mixture-of-experts-pytorch
Simple Modular Mixture of Experts (MoE) model in PyTorch, trained on MNIST with automated data loading, training, evaluation, and visualization.
