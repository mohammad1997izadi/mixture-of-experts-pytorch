# Mixture of Experts (MoE) in PyTorch

This project implements a modular and extensible Mixture of Experts (MoE) architecture using PyTorch, trained on the MNIST dataset.

## ğŸ“ Project Structure

```
moe_project/
â”œâ”€â”€ data/              # MNIST dataset storage
â”œâ”€â”€ models/            # Trained model output
â”œâ”€â”€ scripts/           # Modular training and evaluation scripts
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ main.py            # Entry point
```

## âš™ï¸ Features

- Fully automated pipeline
- Clean modular design with separate concerns
- Visual training metrics (loss and accuracy)
- Easily extendable number of experts
- GPU acceleration supported

## ğŸ§  Architecture

- **Expert Networks**: MLPs trained on MNIST.
- **Gating Network**: Learns weights for combining expert outputs.
- **Mixture Output**: Weighted sum of expert outputs.

## ğŸš€ Getting Started

```bash
pip install -r requirements.txt
python main.py
```

## ğŸ“ˆ Output Example

Training progress is visualized with loss and accuracy plots saved to `models/`.

## ğŸ“œ License

Licensed under the MIT License.
